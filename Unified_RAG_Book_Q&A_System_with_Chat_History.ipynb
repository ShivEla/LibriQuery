{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShivEla/LibriQuery/blob/main/Unified_RAG_Book_Q%26A_System_with_Chat_History.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import os\n",
        "import time # For adding a small delay between API calls\n",
        "\n",
        "# --- Configuration ---\n",
        "OPEN_LIBRARY_API_URL_SEARCH = \"https://openlibrary.org/search.json\"\n",
        "OPEN_LIBRARY_API_URL_WORKS = \"https://openlibrary.org/works/\" # For detailed work data\n",
        "OPEN_LIBRARY_API_URL_AUTHORS = \"https://openlibrary.org/authors/\" # For author data\n",
        "OPEN_LIBRARY_API_URL_BOOKS_BY_BIBKEYS = \"https://openlibrary.org/api/books\" # For ISBN lookup\n",
        "OPEN_LIBRARY_API_URL_COVERS = \"https://covers.openlibrary.org/b/\" # For cover images\n",
        "\n",
        "GEMINI_API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
        "\n",
        "GEMINI_API_KEY = \"YOUR Key\" # <--- REPLACE THIS WITH YOUR ACTUAL API KEY FOR LOCAL USE IF NOT USING SECRETS\n"
      ],
      "metadata": {
        "id": "1OZ1OyIZc2WK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_api_request(url: str, params: dict = None, headers: dict = None) -> dict:\n",
        "    \"\"\"\n",
        "    Helper function to make HTTP GET requests and handle common errors.\n",
        "    \"\"\"\n",
        "    if headers is None:\n",
        "        headers = {}\n",
        "    # headers['User-Agent'] = USER_AGENT # Removed User-Agent as requested\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, params=params, headers=headers)\n",
        "        response.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "        return response.json()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error making API request to {url}: {e}\")\n",
        "        return {}\n"
      ],
      "metadata": {
        "id": "K9ACCgsUdEdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_open_library_data_for_context(query: str, limit: int = 5) -> list:\n",
        "    \"\"\"\n",
        "    Fetches initial book data from the Open Library Search API based on a query.\n",
        "    This is the first step to get potential books for context.\n",
        "    \"\"\"\n",
        "    print(f\"  Searching Open Library for '{query}' as initial context...\")\n",
        "    params = {\"q\": query, \"limit\": limit}\n",
        "    data = make_api_request(OPEN_LIBRARY_API_URL_SEARCH, params=params)\n",
        "    return data.get(\"docs\", [])"
      ],
      "metadata": {
        "id": "evklZ1N1dGiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_detailed_work_data(work_olid: str) -> dict:\n",
        "    \"\"\"\n",
        "    Fetches more detailed data for a specific book work using its OLID.\n",
        "    This uses the /works/{OLID}.json API.\n",
        "    \"\"\"\n",
        "    url = f\"{OPEN_LIBRARY_API_URL_WORKS}{work_olid}.json\"\n",
        "    data = make_api_request(url)\n",
        "\n",
        "    detailed_info = {\n",
        "        'description': 'No detailed description available.',\n",
        "        'subjects': [],\n",
        "        'authors_ol_ids': [] # To store author OLIDs\n",
        "    }\n",
        "\n",
        "    description = data.get('description')\n",
        "    if isinstance(description, dict) and 'value' in description:\n",
        "        detailed_info['description'] = description['value']\n",
        "    elif isinstance(description, str):\n",
        "        detailed_info['description'] = description\n",
        "\n",
        "    subjects = data.get('subjects')\n",
        "    if subjects and isinstance(subjects, list):\n",
        "        detailed_info['subjects'] = subjects\n",
        "\n",
        "    authors = data.get('authors')\n",
        "    if authors and isinstance(authors, list):\n",
        "        for author_entry in authors:\n",
        "            if 'author' in author_entry and 'key' in author_entry['author']:\n",
        "                author_olid = author_entry['author']['key'].split('/')[-1]\n",
        "                detailed_info['authors_ol_ids'].append(author_olid)\n",
        "\n",
        "    return detailed_info"
      ],
      "metadata": {
        "id": "p72wK8j7dLD7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_author_details(author_olid: str) -> dict:\n",
        "    \"\"\"\n",
        "    Fetches detailed data for a specific author using their OLID.\n",
        "    This uses the /authors/{OLID}.json API.\n",
        "    \"\"\"\n",
        "    url = f\"{OPEN_LIBRARY_API_URL_AUTHORS}{author_olid}.json\"\n",
        "    data = make_api_request(url)\n",
        "\n",
        "    author_info = {\n",
        "        'name': data.get('name', 'N/A'),\n",
        "        'bio': 'No biography available.',\n",
        "        'birth_date': data.get('birth_date', 'N/A'),\n",
        "        'death_date': data.get('death_date', 'N/A'),\n",
        "        'author_subjects': data.get('subjects', []) # Subjects associated with the author\n",
        "    }\n",
        "\n",
        "    bio = data.get('bio')\n",
        "    if isinstance(bio, dict) and 'value' in bio:\n",
        "        author_info['bio'] = bio['value']\n",
        "    elif isinstance(bio, str):\n",
        "        author_info['bio'] = bio\n",
        "\n",
        "    return author_info"
      ],
      "metadata": {
        "id": "jTkQGJIidO84"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_book_by_isbn(isbn: str) -> dict:\n",
        "    \"\"\"\n",
        "    Fetches book details using its ISBN.\n",
        "    This uses the /api/books?bibkeys=ISBN:{ISBN} API.\n",
        "    \"\"\"\n",
        "    params = {\"bibkeys\": f\"ISBN:{isbn}\", \"format\": \"json\", \"jscmd\": \"data\"}\n",
        "    data = make_api_request(OPEN_LIBRARY_API_URL_BOOKS_BY_BIBKEYS, params=params)\n",
        "\n",
        "    if data and f\"ISBN:{isbn}\" in data:\n",
        "        book_data = data[f\"ISBN:{isbn}\"]\n",
        "        return {\n",
        "            'isbn_title': book_data.get('title', 'N/A'),\n",
        "            'publish_date': book_data.get('publish_date', 'N/A'),\n",
        "            'number_of_pages': book_data.get('number_of_pages', 'N/A'),\n",
        "            'publishers': [p.get('name') for p in book_data.get('publishers', []) if p.get('name')],\n",
        "            'isbn_authors': [a.get('name') for a in book_data.get('authors', []) if a.get('name')]\n",
        "        }\n",
        "    return {}\n"
      ],
      "metadata": {
        "id": "JY3d3c62dRov"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cover_url(olid: str = None, isbn: str = None, size: str = 'M') -> str:\n",
        "    \"\"\"\n",
        "    Constructs a cover image URL.\n",
        "    Size can be 'S' (small), 'M' (medium), 'L' (large).\n",
        "    Prioritizes OLID if both are provided.\n",
        "    \"\"\"\n",
        "    if olid:\n",
        "        return f\"{OPEN_LIBRARY_API_URL_COVERS}olid/{olid}-{size}.jpg\"\n",
        "    elif isbn:\n",
        "        return f\"{OPEN_LIBRARY_API_URL_COVERS}isbn/{isbn}-{size}.jpg\"\n",
        "    return \"No cover available.\"\n"
      ],
      "metadata": {
        "id": "NVKJ6aAedU8_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_gemini_api(prompt: str, chat_history: list = None, response_schema: dict = None) -> dict:\n",
        "    \"\"\"\n",
        "    Generic function to call the Gemini API with a given prompt, chat history, and an optional response schema.\n",
        "    \"\"\"\n",
        "    # Initialize contents with chat history if provided, otherwise start with just the new prompt\n",
        "    contents = []\n",
        "    if chat_history:\n",
        "        contents.extend(chat_history)\n",
        "    contents.append({\"role\": \"user\", \"parts\": [{\"text\": prompt}]})\n",
        "\n",
        "    payload = {\n",
        "        \"contents\": contents,\n",
        "        \"generationConfig\": {} # Initialize generationConfig\n",
        "    }\n",
        "\n",
        "    if response_schema:\n",
        "        payload[\"generationConfig\"][\"responseMimeType\"] = \"application/json\"\n",
        "        payload[\"generationConfig\"][\"responseSchema\"] = response_schema\n",
        "    else:\n",
        "        # For plain text, ensure no responseMimeType is set for JSON\n",
        "        pass # Default is text/plain\n",
        "\n",
        "    headers = {'Content-Type': 'application/json'}\n",
        "    try:\n",
        "        # Ensure GEMINI_API_KEY is not empty before making the call\n",
        "        if not GEMINI_API_KEY or GEMINI_API_KEY == \"YOUR_GEMINI_API_KEY\":\n",
        "            raise ValueError(\"GEMINI_API_KEY is not set. Please provide your API key.\")\n",
        "\n",
        "        response = requests.post(GEMINI_API_URL, headers=headers, data=json.dumps(payload), params={\"key\": GEMINI_API_KEY})\n",
        "        response.raise_for_status()\n",
        "        result = response.json()\n",
        "\n",
        "        if result.get(\"candidates\") and len(result[\"candidates\"]) > 0 and \\\n",
        "           result[\"candidates\"][0].get(\"content\") and \\\n",
        "           result[\"candidates\"][0][\"content\"].get(\"parts\") and \\\n",
        "           len(result[\"candidates\"][0][\"content\"][\"parts\"]) > 0:\n",
        "            if response_schema:\n",
        "                json_string = result[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "                return json.loads(json_string)\n",
        "            else:\n",
        "                # For plain text responses\n",
        "                return {\"text\": result[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]}\n",
        "        else:\n",
        "            print(f\"Warning: LLM response was empty or malformed.\")\n",
        "            return {}\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error calling Gemini API: {e}\")\n",
        "        return {}\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error decoding JSON from LLM response: {e}\")\n",
        "        # print(f\"Raw LLM response: {response.text}\") # Uncomment for debugging raw response\n",
        "        return {}\n",
        "    except ValueError as e:\n",
        "        print(f\"Configuration Error: {e}\")\n",
        "        return {}\n"
      ],
      "metadata": {
        "id": "ao92NW1rdZeD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_search_terms_from_question(question: str, chat_history: list = None) -> str:\n",
        "    \"\"\"\n",
        "    Uses LLM to extract relevant search terms from a user's question, considering chat history.\n",
        "    This helps in dynamically searching Open Library for context.\n",
        "    Includes a basic fallback if LLM extraction fails.\n",
        "    \"\"\"\n",
        "    history_str = \"\"\n",
        "    if chat_history:\n",
        "        for msg in chat_history:\n",
        "            history_str += f\"{msg['role'].capitalize()}: {msg['parts'][0]['text']}\\n\"\n",
        "\n",
        "    prompt = f\"\"\"Given the following conversation history and the new user question, identify the primary book title, author name, or series name that would be most effective for searching Open Library. If the question is about a general topic or genre, extract that.\n",
        "    Return only the most relevant single keyword or a short phrase for Open Library search. Do NOT include words like 'plot', 'storyline', 'summary', 'rating', 'review', 'what is the', 'tell me about' in the extracted terms.\n",
        "    If no specific book/author/series is mentioned, return a relevant genre or 'book'.\n",
        "\n",
        "    Chat History:\n",
        "    {history_str}\n",
        "\n",
        "    User Question: \"{question}\"\n",
        "\n",
        "    Search Term:\"\"\"\n",
        "\n",
        "    response = call_gemini_api(prompt, chat_history=chat_history) # Pass history to LLM call\n",
        "    extracted_term = response.get('text', '').strip()\n",
        "\n",
        "    # Basic cleanup for common phrases that LLM might still include\n",
        "    extracted_term_lower = extracted_term.lower()\n",
        "    for phrase in [\"plot of\", \"storyline of\", \"summary of\", \"rating of\", \"review of\", \"what is the\", \"tell me about\"]:\n",
        "        if extracted_term_lower.startswith(phrase):\n",
        "            extracted_term = extracted_term_lower.replace(phrase, \"\", 1).strip()\n",
        "\n",
        "    if not extracted_term:\n",
        "        question_lower = question.lower()\n",
        "        if \"plot of\" in question_lower:\n",
        "            return question_lower.split(\"plot of\", 1)[1].strip().replace(\"?\", \"\").replace(\".\", \"\")\n",
        "        elif \"storyline of\" in question_lower:\n",
        "            return question_lower.split(\"storyline of\", 1)[1].strip().replace(\"?\", \"\").replace(\".\", \"\")\n",
        "        return \"book\"\n",
        "\n",
        "    return extracted_term\n"
      ],
      "metadata": {
        "id": "PSfPHFTSdcyB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "def answer_question_about_books(user_question: str, books_context: list, chat_history: list = None) -> str:\n",
        "    \"\"\"\n",
        "    Uses an LLM to answer a user's question based on the provided book summaries, genres, and author details,\n",
        "    considering chat history.\n",
        "\n",
        "    Args:\n",
        "        user_question (str): The question asked by the user.\n",
        "        books_context (list): A list of dictionaries, each containing enriched book data.\n",
        "        chat_history (list): The history of the conversation.\n",
        "\n",
        "    Returns:\n",
        "        str: The answer generated by the LLM.\n",
        "    \"\"\"\n",
        "    if not books_context:\n",
        "        return \"I couldn't find any relevant book information to answer your question. Please try a different query.\"\n",
        "\n",
        "    context_text = []\n",
        "    for book in books_context:\n",
        "        title = book.get('title', 'N/A')\n",
        "        author = \", \".join(book.get('author_name', ['N/A']))\n",
        "        summary = book.get('description', book.get('first_sentence', ['No description available.'])[0])\n",
        "        genres = \", \".join(book.get('subjects', book.get('subject', ['N/A'])))\n",
        "        publish_date = book.get('publish_date', 'N/A')\n",
        "        num_pages = book.get('number_of_pages', 'N/A')\n",
        "        publishers = \", \".join(book.get('publishers', ['N/A']))\n",
        "        cover_url = book.get('cover_url', 'N/A')\n",
        "\n",
        "        author_bio = book.get('author_bio', 'No author biography available.')\n",
        "        author_birth_date = book.get('author_birth_date', 'N/A')\n",
        "        author_death_date = book.get('author_death_date', 'N/A')\n",
        "\n",
        "        context_text.append(f\"Book Title: \\\"{title}\\\"\\nAuthor: \\\"{author}\\\"\\nAuthor Bio: \\\"{author_bio}\\\"\\nAuthor Birth Date: {author_birth_date}\\nAuthor Death Date: {author_death_date}\\nSummary: \\\"{summary}\\\"\\nGenres: \\\"{genres}\\\"\\nPublish Date: {publish_date}\\nNumber of Pages: {num_pages}\\nPublishers: {publishers}\\nCover URL: {cover_url}\\n---\")\n",
        "\n",
        "    full_context = \"\\n\\n\".join(context_text)\n",
        "\n",
        "    prompt = f\"\"\"Based on the following detailed book and author information, and the conversation history, answer the user's question concisely. Use all provided details including summaries, genres, author bios, publish dates, pages, and publishers. If the exact information (e.g., specific plot details not in summary, or precise numerical ratings if not provided in the context) is not present in the provided context, state that you cannot answer based on the given books.\n",
        "\n",
        "Book and Author Information:\n",
        "{full_context}\n",
        "\n",
        "User Question: \"{user_question}\"\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    response = call_gemini_api(prompt, chat_history=chat_history) # Pass history to LLM call\n",
        "    return response.get('text', 'Could not generate an answer.')\n",
        "\n",
        "\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                        LIBRIQUERY                        \n",
            "\n",
            "Ask any question about books, and I'll try to answer based on Open Library data.\n",
            "\n",
            "Type 'exit' to quit.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-2321795062.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2-2321795062.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m         \u001b[0muser_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nYour Question (or 'exit' to quit): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_question\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Exiting Q&A session. Goodbye!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "collapsed": true,
        "id": "BW48ZVQDcaV4",
        "outputId": "b1c5c2e8-98bf-45ef-b2a3-bbeee80ee26d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print(\"                        LIBRIQUERY                        \")\n",
        "    print(\"\\nAsk any question about books, and I'll try to answer based on Open Library data.\")\n",
        "    print(\"\\nType 'exit' to quit.\")\n",
        "\n",
        "    # Initialize chat history\n",
        "    chat_history = []\n",
        "\n",
        "    while True:\n",
        "        user_question = input(\"\\nYour Question (or 'exit' to quit): \")\n",
        "        if user_question.lower() == 'exit':\n",
        "            print(\"Exiting Q&A session. Goodbye!\")\n",
        "            break\n",
        "        if not user_question.strip():\n",
        "            print(\"Please enter a question.\")\n",
        "            continue\n",
        "\n",
        "        # Add user's question to chat history\n",
        "        chat_history.append({\"role\": \"user\", \"parts\": [{\"text\": user_question}]})\n",
        "\n",
        "        # Step 1: Extract search terms from the user's question using LLM, considering chat history\n",
        "        search_terms = extract_search_terms_from_question(user_question, chat_history=chat_history)\n",
        "        print(f\"Identified search terms: '{search_terms}'\")\n",
        "\n",
        "        # Fetch initial book data from Open Library based on extracted terms\n",
        "        initial_search_results = fetch_open_library_data_for_context(search_terms)\n",
        "\n",
        "        if not initial_search_results:\n",
        "            print(\"No books/authors found matching your query for context. Please try a different question.\")\n",
        "            # If no new context, still try to answer from previous context if available\n",
        "            answer = answer_question_about_books(user_question, [], chat_history=chat_history)\n",
        "            print(f\"\\nAnswer: {answer}\")\n",
        "            chat_history.append({\"role\": \"model\", \"parts\": [{\"text\": answer}]})\n",
        "            continue\n",
        "\n",
        "        # Enrich book data with more details from various Open Library APIs\n",
        "        print(\"Enriching book data with more details from multiple Open Library APIs...\")\n",
        "        books_for_qa_context = []\n",
        "        for i, book in enumerate(initial_search_results):\n",
        "            #print(f\"    Enriching book {i+1}/{len(initial_search_results)}: {book.get('title', 'N/A')}\")\n",
        "\n",
        "            # Fetch detailed work data\n",
        "            work_olid = book.get('key', '').replace('/works/', '')\n",
        "            if work_olid:\n",
        "                detailed_work_data = fetch_detailed_work_data(work_olid)\n",
        "                book.update(detailed_work_data)\n",
        "                time.sleep(0.1) # Small delay\n",
        "\n",
        "            # Fetch author details if OLID is available from work data\n",
        "            author_ol_ids = book.get('authors_ol_ids', [])\n",
        "            if author_ol_ids:\n",
        "                # For simplicity, just fetch details for the first author found\n",
        "                first_author_olid = author_ol_ids[0]\n",
        "                author_details = fetch_author_details(first_author_olid)\n",
        "                book['author_bio'] = author_details.get('bio')\n",
        "                book['author_birth_date'] = author_details.get('birth_date')\n",
        "                book['author_death_date'] = author_details.get('death_date')\n",
        "                book['author_subjects'] = author_details.get('author_subjects') # Add author's subjects\n",
        "                time.sleep(0.1) # Small delay\n",
        "\n",
        "            # Fetch ISBN-specific data if ISBNs are available\n",
        "            isbns = book.get('isbn', [])\n",
        "            if isbns:\n",
        "                # Use the first ISBN found for detailed lookup\n",
        "                first_isbn = isbns[0]\n",
        "                isbn_data = fetch_book_by_isbn(first_isbn)\n",
        "                book.update(isbn_data) # Merge ISBN-specific data\n",
        "                time.sleep(0.1) # Small delay\n",
        "\n",
        "            # Construct cover URL\n",
        "            book['cover_url'] = get_cover_url(olid=book.get('cover_edition_key', '').replace('/books/', '') or book.get('olid'), isbn=isbns[0] if isbns else None)\n",
        "\n",
        "            books_for_qa_context.append(book)\n",
        "\n",
        "        # Use the enriched book data to answer the user's question\n",
        "        print(\"Generating answer based on retrieved and enriched book information...\")\n",
        "        answer = answer_question_about_books(user_question, books_for_qa_context, chat_history=chat_history)\n",
        "        print(f\"\\nAnswer: {answer}\")\n",
        "\n",
        "        # Add AI's answer to chat history\n",
        "        chat_history.append({\"role\": \"model\", \"parts\": [{\"text\": answer}]})\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "3SY1NB4Ado5R"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShivEla/LibriQuery/blob/main/LibriQuery_RAG_Book_Q%26A_System_(LangChain_FAISS_Gemini).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-google-genai langchain-community python-dotenv requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooqLcTZySIOa",
        "outputId": "e752a711-fc3a-476c-bc26-985fe7fac5a4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.11/dist-packages (2.1.7)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.18 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.6.18)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.68 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.3.68)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (2.11.7)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.26)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.4)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.7.9)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.29.5)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (4.14.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.73.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Install necessary libraries in Colab\n",
        "!pip install langchain-google-genai langchain-community python-dotenv requests faiss-cpu\n",
        "\n",
        "import requests\n",
        "import json\n",
        "import os\n",
        "import time # For adding a small delay between API calls\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# LangChain Imports for Gemini\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.documents import Document # For creating Document objects for FAISS\n",
        "\n",
        "# Load environment variables from .env file (for GEMINI_API_KEY)\n",
        "load_dotenv()\n",
        "\n",
        "# --- Configuration ---\n",
        "OPEN_LIBRARY_API_URL_SEARCH = \"https://openlibrary.org/search.json\"\n",
        "OPEN_LIBRARY_API_URL_WORKS = \"https://openlibrary.org/works/\" # For detailed work data\n",
        "OPEN_LIBRARY_API_URL_AUTHORS = \"https://openlibrary.org/authors/\" # For author data\n",
        "OPEN_LIBRARY_API_URL_BOOKS_BY_BIBKEYS = \"https://openlibrary.org/api/books\" # For ISBN lookup\n",
        "OPEN_LIBRARY_API_URL_COVERS = \"https://covers.openlibrary.org/b/\" # For cover images\n",
        "\n",
        "# Gemini API Key - used for both embeddings and the ChatGoogleGenerativeAI LLM\n",
        "# Ensure GEMINI_API_KEY is set in your environment variables or .env file\n",
        "# Example: GEMINI_API_KEY=\"AIzaSyC...\"\n",
        "GEMINI_API_KEY = \"AIzaSyC95Y7RY-tIx6vg8Hm2nZifct1JSq63ffA\" # <--- REPLACE THIS WITH YOUR ACTUAL API KEY FOR LOCAL USE\n",
        "\n",
        "\n",
        "# --- Helper Functions for Open Library Data Retrieval ---\n",
        "\n",
        "def make_api_request(url: str, params: dict = None, headers: dict = None) -> dict:\n",
        "    \"\"\"\n",
        "    Helper function to make HTTP GET requests and handle common errors.\n",
        "    \"\"\"\n",
        "    if headers is None:\n",
        "        headers = {}\n",
        "    # User-Agent removed as per previous request.\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, params=params, headers=headers)\n",
        "        response.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "        return response.json()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error making API request to {url}: {e}\")\n",
        "        return {}\n",
        "\n",
        "def fetch_open_library_data_for_context(query: str, limit: int = 5) -> list:\n",
        "    \"\"\"\n",
        "    Fetches initial book data from the Open Library Search API based on a query.\n",
        "    \"\"\"\n",
        "    print(f\"  Searching Open Library for '{query}' as initial context...\")\n",
        "    params = {\"q\": query, \"limit\": limit}\n",
        "    data = make_api_request(OPEN_LIBRARY_API_URL_SEARCH, params=params)\n",
        "    return data.get(\"docs\", [])\n",
        "\n",
        "def fetch_detailed_work_data(work_olid: str) -> dict:\n",
        "    \"\"\"\n",
        "    Fetches more detailed data for a specific book work using its OLID.\n",
        "    This uses the /works/{OLID}.json API.\n",
        "    \"\"\"\n",
        "    url = f\"{OPEN_LIBRARY_API_URL_WORKS}{work_olid}.json\"\n",
        "    data = make_api_request(url)\n",
        "\n",
        "    detailed_info = {\n",
        "        'description': 'No detailed description available.',\n",
        "        'subjects': [],\n",
        "        'authors_ol_ids': [] # To store author OLIDs\n",
        "    }\n",
        "\n",
        "    description = data.get('description')\n",
        "    if isinstance(description, dict) and 'value' in description:\n",
        "        detailed_info['description'] = description['value']\n",
        "    elif isinstance(description, str):\n",
        "        detailed_info['description'] = description\n",
        "\n",
        "    subjects = data.get('subjects')\n",
        "    if subjects and isinstance(subjects, list):\n",
        "        detailed_info['subjects'] = subjects\n",
        "\n",
        "    authors = data.get('authors')\n",
        "    if authors and isinstance(authors, list):\n",
        "        for author_entry in authors:\n",
        "            if 'author' in author_entry and 'key' in author_entry['author']:\n",
        "                author_olid = author_entry['author']['key'].split('/')[-1]\n",
        "                detailed_info['authors_ol_ids'].append(author_olid)\n",
        "\n",
        "    return detailed_info\n",
        "\n",
        "def fetch_author_details(author_olid: str) -> dict:\n",
        "    \"\"\"\n",
        "    Fetches detailed data for a specific author using their OLID.\n",
        "    This uses the /authors/{OLID}.json API.\n",
        "    \"\"\"\n",
        "    url = f\"{OPEN_LIBRARY_API_URL_AUTHORS}{author_olid}.json\"\n",
        "    data = make_api_request(url)\n",
        "\n",
        "    author_info = {\n",
        "        'name': data.get('name', 'N/A'),\n",
        "        'bio': 'No biography available.',\n",
        "        'birth_date': data.get('birth_date', 'N/A'),\n",
        "        'death_date': data.get('death_date', 'N/A'),\n",
        "        'author_subjects': data.get('subjects', []) # Subjects associated with the author\n",
        "    }\n",
        "\n",
        "    bio = data.get('bio')\n",
        "    if isinstance(bio, dict) and 'value' in bio:\n",
        "        author_info['bio'] = bio['value']\n",
        "    elif isinstance(bio, str):\n",
        "        author_info['bio'] = bio\n",
        "\n",
        "    return author_info\n",
        "\n",
        "def fetch_book_by_isbn(isbn: str) -> dict:\n",
        "    \"\"\"\n",
        "    Fetches book details using its ISBN.\n",
        "    This uses the /api/books?bibkeys=ISBN:{ISBN} API.\n",
        "    \"\"\"\n",
        "    params = {\"bibkeys\": f\"ISBN:{isbn}\", \"format\": \"json\", \"jscmd\": \"data\"}\n",
        "    data = make_api_request(OPEN_LIBRARY_API_URL_BOOKS_BY_BIBKEYS, params=params)\n",
        "\n",
        "    if data and f\"ISBN:{isbn}\" in data:\n",
        "        book_data = data[f\"ISBN:{isbn}\"]\n",
        "        return {\n",
        "            'isbn_title': book_data.get('title', 'N/A'),\n",
        "            'publish_date': book_data.get('publish_date', 'N/A'),\n",
        "            'number_of_pages': book_data.get('number_of_pages', 'N/A'),\n",
        "            'publishers': [p.get('name') for p in book_data.get('publishers', []) if p.get('name')],\n",
        "            'isbn_authors': [a.get('name') for a in book_data.get('authors', []) if a.get('name')]\n",
        "        }\n",
        "    return {}\n",
        "\n",
        "def get_cover_url(olid: str = None, isbn: str = None, size: str = 'M') -> str:\n",
        "    \"\"\"\n",
        "    Constructs a cover image URL.\n",
        "    Size can be 'S' (small), 'M' (medium), 'L' (large).\n",
        "    Prioritizes OLID if both are provided.\n",
        "    \"\"\"\n",
        "    if olid:\n",
        "        return f\"{OPEN_LIBRARY_API_URL_COVERS}olid/{olid}-{size}.jpg\"\n",
        "    elif isbn:\n",
        "        return f\"{OPEN_LIBRARY_API_URL_COVERS}isbn/{isbn}-{size}.jpg\"\n",
        "    return \"No cover available.\"\n",
        "\n",
        "# --- RAG Core Functions (using LangChain with Gemini) ---\n",
        "\n",
        "def create_faiss_index(books_data: list, embeddings_model: GoogleGenerativeAIEmbeddings) -> FAISS:\n",
        "    \"\"\"\n",
        "    Creates a FAISS vector store from enriched book data.\n",
        "    \"\"\"\n",
        "    documents = []\n",
        "    for book in books_data:\n",
        "        # Combine relevant information into page_content for embedding\n",
        "        content = f\"Title: {book.get('title', 'N/A')}\\n\" \\\n",
        "                  f\"Author: {', '.join(book.get('author_name', ['N/A']))}\\n\" \\\n",
        "                  f\"Description: {book.get('description', book.get('first_sentence', ['No description available.'])[0])}\\n\" \\\n",
        "                  f\"Genres: {', '.join(book.get('subjects', book.get('subject', ['N/A'])))}\\n\" \\\n",
        "                  f\"Publish Date: {book.get('publish_date', 'N/A')}\\n\" \\\n",
        "                  f\"Number of Pages: {book.get('number_of_pages', 'N/A')}\\n\" \\\n",
        "                  f\"Publishers: {', '.join(book.get('publishers', ['N/A']))}\\n\" \\\n",
        "                  f\"Author Bio: {book.get('author_bio', 'No author biography available.')}\\n\" \\\n",
        "                  f\"Author Birth Date: {book.get('author_birth_date', 'N/A')}\\n\" \\\n",
        "                  f\"Author Death Date: {book.get('author_death_date', 'N/A')}\"\n",
        "\n",
        "        # Store useful metadata for source attribution\n",
        "        metadata = {\n",
        "            \"title\": book.get('title', 'N/A'),\n",
        "            \"author\": ', '.join(book.get('author_name', ['N/A'])),\n",
        "            \"olid\": book.get('key', '').replace('/works/', '') # Use work OLID as a stable ID\n",
        "        }\n",
        "        documents.append(Document(page_content=content, metadata=metadata))\n",
        "\n",
        "    if not documents:\n",
        "        raise ValueError(\"No documents generated to create FAISS index.\")\n",
        "\n",
        "    print(\"  Creating FAISS vector store...\")\n",
        "    vectorstore = FAISS.from_documents(documents, embeddings_model)\n",
        "    print(\"  FAISS vector store created.\")\n",
        "    return vectorstore\n",
        "\n",
        "def extract_search_terms_from_question(question: str, llm_model: ChatGoogleGenerativeAI) -> str:\n",
        "    \"\"\"\n",
        "    Uses LLM to extract relevant search terms from a user's question.\n",
        "    Now uses LangChain's ChatGoogleGenerativeAI.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"From the following user question, identify the primary book title, author name, or series name that would be most effective for searching Open Library. If the question is about a general topic or genre, extract that.\n",
        "    Return only the most relevant single keyword or a short phrase for Open Library search. Do NOT include words like 'plot', 'storyline', 'summary', 'rating', 'review', 'what is the', 'tell me about' in the extracted terms.\n",
        "    If no specific book/author/series is mentioned, return a relevant genre or 'book'.\n",
        "\n",
        "    User Question: \"{question}\"\n",
        "\n",
        "    Search Term:\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Using LLM directly for extraction\n",
        "        response = llm_model.invoke(prompt)\n",
        "        extracted_term = response.content.strip()\n",
        "\n",
        "        # Basic cleanup for common phrases that LLM might still include\n",
        "        extracted_term_lower = extracted_term.lower()\n",
        "        for phrase in [\"plot of\", \"storyline of\", \"summary of\", \"rating of\", \"review of\", \"what is the\", \"tell me about\"]:\n",
        "            if extracted_term_lower.startswith(phrase):\n",
        "                extracted_term = extracted_term_lower.replace(phrase, \"\", 1).strip()\n",
        "\n",
        "        if not extracted_term:\n",
        "            question_lower = question.lower()\n",
        "            if \"plot of\" in question_lower:\n",
        "                return question_lower.split(\"plot of\", 1)[1].strip().replace(\"?\", \"\").replace(\".\", \"\")\n",
        "            elif \"storyline of\" in question_lower:\n",
        "                return question_lower.split(\"storyline of\", 1)[1].strip().replace(\"?\", \"\").replace(\".\", \"\")\n",
        "            return \"book\" # Fallback\n",
        "        return extracted_term\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting search terms with LLM: {e}. Falling back to simple parsing.\")\n",
        "        # Fallback if LLM extraction fails\n",
        "        question_lower = question.lower()\n",
        "        if \"plot of\" in question_lower:\n",
        "            return question_lower.split(\"plot of\", 1)[1].strip().replace(\"?\", \"\").replace(\".\", \"\")\n",
        "        elif \"storyline of\" in question_lower:\n",
        "            return question_lower.split(\"storyline of\", 1)[1].strip().replace(\"?\", \"\").replace(\".\", \"\")\n",
        "        return \"book\" # Final fallback\n",
        "\n",
        "\n",
        "# Define custom prompt templates for the ConversationalRetrievalChain\n",
        "qa_prompt_template = \"\"\"\n",
        "You are a helpful chatbot that answers questions about books.\n",
        "Use ONLY the following pieces of context to answer the question at the end.\n",
        "If the information is not explicitly mentioned in the context, say \"I don't have specific information about that.\"\n",
        "Do not make up or infer information that is not directly stated in the context.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Chat History:\n",
        "{chat_history}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "QA_PROMPT = PromptTemplate(\n",
        "    template=qa_prompt_template,\n",
        "    input_variables=[\"context\", \"question\", \"chat_history\"]\n",
        ")\n",
        "\n",
        "# --- Main Application Logic ---\n",
        "\n",
        "def main():\n",
        "    print(\"--- LibriQuery: Command-Line RAG Book Q&A System ---\")\n",
        "    print(\"Ask any question about books, and I'll try to answer based on Open Library data.\")\n",
        "    print(\"Type 'exit' to quit.\")\n",
        "\n",
        "    # Initialize LangChain components once\n",
        "    # These will be re-initialized if a new FAISS index is created\n",
        "    # Changed model to gemini-1.5-flash for broader availability\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GEMINI_API_KEY)\n",
        "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.7, google_api_key=GEMINI_API_KEY)\n",
        "    memory = ConversationBufferMemory(\n",
        "        memory_key=\"chat_history\",\n",
        "        return_messages=True,\n",
        "        output_key=\"answer\"\n",
        "    )\n",
        "\n",
        "    # Initialize conversation_chain outside the loop, but it needs a retriever\n",
        "    # which depends on the FAISS index. So, we'll initialize it after fetching data.\n",
        "    conversation_chain = None\n",
        "\n",
        "    # Store fetched books globally for the session\n",
        "    global_books_context = []\n",
        "\n",
        "    while True:\n",
        "        user_question = input(\"\\nYour Question (or 'exit' to quit): \")\n",
        "        if user_question.lower() == 'exit':\n",
        "            print(\"Exiting Q&A session. Goodbye!\")\n",
        "            break\n",
        "        if not user_question.strip():\n",
        "            print(\"Please enter a question.\")\n",
        "            continue\n",
        "\n",
        "        print(\"Understanding your question and searching for context...\")\n",
        "\n",
        "        # Step 1: Extract search terms from the user's question using LLM\n",
        "        search_terms = extract_search_terms_from_question(user_question, llm)\n",
        "        print(f\"Identified search terms: '{search_terms}'\")\n",
        "\n",
        "        # Step 2: Fetch initial book data from Open Library based on extracted terms\n",
        "        initial_search_results = fetch_open_library_data_for_context(search_terms)\n",
        "\n",
        "        if not initial_search_results:\n",
        "            print(\"No books found matching your query for context. Please try a different question.\")\n",
        "            continue\n",
        "\n",
        "        # Step 3: Enrich book data with more details from various Open Library APIs\n",
        "        print(\"  Enriching book data with more details from multiple Open Library APIs...\")\n",
        "        books_for_qa_context = []\n",
        "        for i, book in enumerate(initial_search_results):\n",
        "            print(f\"    Enriching book {i+1}/{len(initial_search_results)}: {book.get('title', 'N/A')}\")\n",
        "\n",
        "            # Fetch detailed work data\n",
        "            work_olid = book.get('key', '').replace('/works/', '')\n",
        "            if work_olid:\n",
        "                detailed_work_data = fetch_detailed_work_data(work_olid)\n",
        "                book.update(detailed_work_data)\n",
        "                time.sleep(0.05) # Small delay\n",
        "\n",
        "            # Fetch author details if OLID is available from work data\n",
        "            author_ol_ids = book.get('authors_ol_ids', [])\n",
        "            if author_ol_ids:\n",
        "                # For simplicity, just fetch details for the first author found\n",
        "                first_author_olid = author_ol_ids[0]\n",
        "                author_details = fetch_author_details(first_author_olid)\n",
        "                book['author_bio'] = author_details.get('bio')\n",
        "                book['author_birth_date'] = author_details.get('birth_date')\n",
        "                book['author_death_date'] = author_details.get('death_date')\n",
        "                book['author_subjects'] = author_details.get('author_subjects') # Add author's subjects\n",
        "                time.sleep(0.05) # Small delay\n",
        "\n",
        "            # Fetch ISBN-specific data if ISBNs are available\n",
        "            isbns = book.get('isbn', [])\n",
        "            if isbns:\n",
        "                # Use the first ISBN found for detailed lookup\n",
        "                first_isbn = isbns[0]\n",
        "                isbn_data = fetch_book_by_isbn(first_isbn)\n",
        "                book.update(isbn_data) # Merge ISBN-specific data\n",
        "                time.sleep(0.05) # Small delay\n",
        "\n",
        "            # Construct cover URL\n",
        "            book['cover_url'] = get_cover_url(olid=book.get('cover_edition_key', '').replace('/books/', '') or book.get('olid'), isbn=isbns[0] if isbns else None)\n",
        "\n",
        "            books_for_qa_context.append(book)\n",
        "\n",
        "        # Update global context\n",
        "        global_books_context = books_for_qa_context\n",
        "\n",
        "        # Step 4: Create FAISS index and RAG chain from the enriched data\n",
        "        try:\n",
        "            vectorstore = create_faiss_index(global_books_context, embeddings)\n",
        "            retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
        "\n",
        "            # Re-initialize conversation chain with the new retriever\n",
        "            conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "                llm=llm,\n",
        "                retriever=retriever,\n",
        "                memory=memory,\n",
        "                combine_docs_chain_kwargs={\"prompt\": QA_PROMPT},\n",
        "                return_source_documents=True\n",
        "            )\n",
        "            print(\"  RAG chain initialized with new context.\")\n",
        "\n",
        "            # Step 5: Answer the user's question using the RAG chain\n",
        "            print(\"Generating answer using RAG chain...\")\n",
        "            result = conversation_chain({\"question\": user_question})\n",
        "            answer = result[\"answer\"]\n",
        "            source_documents = result[\"source_documents\"]\n",
        "\n",
        "            print(\"\\nAnswer:\", answer)\n",
        "            print(\"\\nSources:\")\n",
        "            if source_documents:\n",
        "                for i, doc in enumerate(source_documents[:3]): # Show only top 3 sources\n",
        "                    # Displaying metadata for context\n",
        "                    source_title = doc.metadata.get('title', 'Unknown Title')\n",
        "                    source_author = doc.metadata.get('author', 'Unknown Author')\n",
        "                    print(f\"Source {i + 1}: Title: '{source_title}', Author: '{source_author}'\")\n",
        "            else:\n",
        "                print(\"No specific sources found in the retrieved context.\")\n",
        "\n",
        "        except ValueError as e:\n",
        "            print(f\"Error during RAG process: {e}\")\n",
        "            print(\"Please ensure your Gemini API key is correct and try a different query.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred during RAG process: {e}\")\n",
        "            print(\"Please try again.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkTunMxnTUs8",
        "outputId": "cef53b29-f767-4468-e074-6fa8fd39dce7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.11/dist-packages (2.1.7)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.18 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.6.18)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.68 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.3.68)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (2.11.7)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.26)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.4)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.7.9)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.29.5)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (4.14.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.73.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.11.0\n",
            "--- LibriQuery: Command-Line RAG Book Q&A System ---\n",
            "Ask any question about books, and I'll try to answer based on Open Library data.\n",
            "Type 'exit' to quit.\n",
            "\n",
            "Your Question (or 'exit' to quit): chetan bhagat\n",
            "Understanding your question and searching for context...\n",
            "Identified search terms: 'Chetan Bhagat'\n",
            "  Searching Open Library for 'Chetan Bhagat' as initial context...\n",
            "  Enriching book data with more details from multiple Open Library APIs...\n",
            "    Enriching book 1/5: Half Girlfriend Chetan Bhagat\n",
            "    Enriching book 2/5: three mistakes of my life by chetan bhagat\n",
            "    Enriching book 3/5: 2 states\n",
            "    Enriching book 4/5: Revolution 2020\n",
            "    Enriching book 5/5: The 3 Mistakes of My Life\n",
            "  Creating FAISS vector store...\n",
            "  FAISS vector store created.\n",
            "  RAG chain initialized with new context.\n",
            "Generating answer using RAG chain...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-6-3143666149.py:357: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  result = conversation_chain({\"question\": user_question})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Answer: Chetan Bhagat is the author of *Half Girlfriend* and *2 States*.  His birth date is 1974.  I don't have specific information about his death date or a biography.\n",
            "\n",
            "Sources:\n",
            "Source 1: Title: 'three mistakes of my life by chetan bhagat', Author: 'N/A'\n",
            "Source 2: Title: 'Half Girlfriend Chetan Bhagat', Author: 'Chetan Bhagat'\n",
            "Source 3: Title: '2 states', Author: 'Chetan Bhagat'\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
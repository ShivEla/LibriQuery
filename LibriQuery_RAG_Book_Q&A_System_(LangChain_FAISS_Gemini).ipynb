{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShivEla/LibriQuery/blob/main/LibriQuery_RAG_Book_Q%26A_System_(LangChain_FAISS_Gemini).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-google-genai langchain-community python-dotenv requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ooqLcTZySIOa",
        "outputId": "e752a711-fc3a-476c-bc26-985fe7fac5a4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.11/dist-packages (2.1.7)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.18 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.6.18)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.68 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.3.68)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (2.11.7)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.26)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.4)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.7.9)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.29.5)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (4.14.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.73.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Install necessary libraries in Colab\n",
        "!pip install langchain-google-genai langchain-community python-dotenv requests faiss-cpu\n",
        "\n",
        "import requests\n",
        "import json\n",
        "import os\n",
        "import time # For adding a small delay between API calls\n",
        "from dotenv import load_dotenv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "v1S1cd6CUj4q",
        "outputId": "0345776d-0ac7-445e-8f16-e86a5c13da70"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.11/dist-packages (2.1.7)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.18 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.6.18)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.68 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.3.68)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (2.11.7)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.26)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.4)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.7.9)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.29.5)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (4.14.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.73.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.68->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# LangChain Imports for Gemini\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.documents import Document # For creating Document objects for FAISS\n"
      ],
      "metadata": {
        "id": "89IIbwyCUlGf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Configuration ---\n",
        "OPEN_LIBRARY_API_URL_SEARCH = \"https://openlibrary.org/search.json\"\n",
        "OPEN_LIBRARY_API_URL_WORKS = \"https://openlibrary.org/works/\" # For detailed work data\n",
        "OPEN_LIBRARY_API_URL_AUTHORS = \"https://openlibrary.org/authors/\" # For author data\n",
        "OPEN_LIBRARY_API_URL_BOOKS_BY_BIBKEYS = \"https://openlibrary.org/api/books\" # For ISBN lookup\n",
        "OPEN_LIBRARY_API_URL_COVERS = \"https://covers.openlibrary.org/b/\" # For cover images\n",
        "\n",
        "# Gemini API Key - used for both embeddings and the ChatGoogleGenerativeAI LLM\n",
        "GEMINI_API_KEY = \"YOUR API KEY\" # <--- REPLACE THIS WITH YOUR ACTUAL API KEY FOR LOCAL USE"
      ],
      "metadata": {
        "id": "nvISTtNuUk9G"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_api_request(url: str, params: dict = None, headers: dict = None) -> dict:\n",
        "    \"\"\"\n",
        "    Helper function to make HTTP GET requests and handle common errors.\n",
        "    \"\"\"\n",
        "    if headers is None:\n",
        "        headers = {}\n",
        "    # User-Agent removed as per previous request.\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, params=params, headers=headers)\n",
        "        response.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "        return response.json()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error making API request to {url}: {e}\")\n",
        "        return {}\n"
      ],
      "metadata": {
        "id": "0f9k8aOwU82g"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_open_library_data_for_context(query: str, limit: int = 5) -> list:\n",
        "    \"\"\"\n",
        "    Fetches initial book data from the Open Library Search API based on a query.\n",
        "    \"\"\"\n",
        "    print(f\"  Searching Open Library for '{query}' as initial context...\")\n",
        "    params = {\"q\": query, \"limit\": limit}\n",
        "    data = make_api_request(OPEN_LIBRARY_API_URL_SEARCH, params=params)\n",
        "    return data.get(\"docs\", [])"
      ],
      "metadata": {
        "id": "Y01OhbHTXjr-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_detailed_work_data(work_olid: str) -> dict:\n",
        "    \"\"\"\n",
        "    Fetches more detailed data for a specific book work using its OLID.\n",
        "    This uses the /works/{OLID}.json API.\n",
        "    \"\"\"\n",
        "    url = f\"{OPEN_LIBRARY_API_URL_WORKS}{work_olid}.json\"\n",
        "    data = make_api_request(url)\n",
        "\n",
        "    detailed_info = {\n",
        "        'description': 'No detailed description available.',\n",
        "        'subjects': [],\n",
        "        'authors_ol_ids': [] # To store author OLIDs\n",
        "    }\n",
        "\n",
        "    description = data.get('description')\n",
        "    if isinstance(description, dict) and 'value' in description:\n",
        "        detailed_info['description'] = description['value']\n",
        "    elif isinstance(description, str):\n",
        "        detailed_info['description'] = description\n",
        "\n",
        "    subjects = data.get('subjects')\n",
        "    if subjects and isinstance(subjects, list):\n",
        "        detailed_info['subjects'] = subjects\n",
        "\n",
        "    authors = data.get('authors')\n",
        "    if authors and isinstance(authors, list):\n",
        "        for author_entry in authors:\n",
        "            if 'author' in author_entry and 'key' in author_entry['author']:\n",
        "                author_olid = author_entry['author']['key'].split('/')[-1]\n",
        "                detailed_info['authors_ol_ids'].append(author_olid)\n",
        "\n",
        "    return detailed_info"
      ],
      "metadata": {
        "id": "HrgcKiVrXmlo"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_author_details(author_olid: str) -> dict:\n",
        "    \"\"\"\n",
        "    Fetches detailed data for a specific author using their OLID.\n",
        "    This uses the /authors/{OLID}.json API.\n",
        "    \"\"\"\n",
        "    url = f\"{OPEN_LIBRARY_API_URL_AUTHORS}{author_olid}.json\"\n",
        "    data = make_api_request(url)\n",
        "\n",
        "    author_info = {\n",
        "        'name': data.get('name', 'N/A'),\n",
        "        'bio': 'No biography available.',\n",
        "        'birth_date': data.get('birth_date', 'N/A'),\n",
        "        'death_date': data.get('death_date', 'N/A'),\n",
        "        'author_subjects': data.get('subjects', []) # Subjects associated with the author\n",
        "    }\n",
        "\n",
        "    bio = data.get('bio')\n",
        "    if isinstance(bio, dict) and 'value' in bio:\n",
        "        author_info['bio'] = bio['value']\n",
        "    elif isinstance(bio, str):\n",
        "        author_info['bio'] = bio\n",
        "\n",
        "    return author_info\n"
      ],
      "metadata": {
        "id": "tgGvxeimXsE9"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_book_by_isbn(isbn: str) -> dict:\n",
        "    \"\"\"\n",
        "    Fetches book details using its ISBN.\n",
        "    This uses the /api/books?bibkeys=ISBN:{ISBN} API.\n",
        "    \"\"\"\n",
        "    params = {\"bibkeys\": f\"ISBN:{isbn}\", \"format\": \"json\", \"jscmd\": \"data\"}\n",
        "    data = make_api_request(OPEN_LIBRARY_API_URL_BOOKS_BY_BIBKEYS, params=params)\n",
        "\n",
        "    if data and f\"ISBN:{isbn}\" in data:\n",
        "        book_data = data[f\"ISBN:{isbn}\"]\n",
        "        return {\n",
        "            'isbn_title': book_data.get('title', 'N/A'),\n",
        "            'publish_date': book_data.get('publish_date', 'N/A'),\n",
        "            'number_of_pages': book_data.get('number_of_pages', 'N/A'),\n",
        "            'publishers': [p.get('name') for p in book_data.get('publishers', []) if p.get('name')],\n",
        "            'isbn_authors': [a.get('name') for a in book_data.get('authors', []) if a.get('name')]\n",
        "        }\n",
        "    return {}"
      ],
      "metadata": {
        "id": "ScYQ00UjXuNp"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cover_url(olid: str = None, isbn: str = None, size: str = 'M') -> str:\n",
        "    \"\"\"\n",
        "    Constructs a cover image URL.\n",
        "    Size can be 'S' (small), 'M' (medium), 'L' (large).\n",
        "    Prioritizes OLID if both are provided.\n",
        "    \"\"\"\n",
        "    if olid:\n",
        "        return f\"{OPEN_LIBRARY_API_URL_COVERS}olid/{olid}-{size}.jpg\"\n",
        "    elif isbn:\n",
        "        return f\"{OPEN_LIBRARY_API_URL_COVERS}isbn/{isbn}-{size}.jpg\"\n",
        "    return \"No cover available.\""
      ],
      "metadata": {
        "id": "UNF_yqIoXxan"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_faiss_index(books_data: list, embeddings_model: GoogleGenerativeAIEmbeddings) -> FAISS:\n",
        "    \"\"\"\n",
        "    Creates a FAISS vector store from enriched book data.\n",
        "    \"\"\"\n",
        "    documents = []\n",
        "    for book in books_data:\n",
        "        # Combine relevant information into page_content for embedding\n",
        "        content = f\"Title: {book.get('title', 'N/A')}\\n\" \\\n",
        "                  f\"Author: {', '.join(book.get('author_name', ['N/A']))}\\n\" \\\n",
        "                  f\"Description: {book.get('description', book.get('first_sentence', ['No description available.'])[0])}\\n\" \\\n",
        "                  f\"Genres: {', '.join(book.get('subjects', book.get('subject', ['N/A'])))}\\n\" \\\n",
        "                  f\"Publish Date: {book.get('publish_date', 'N/A')}\\n\" \\\n",
        "                  f\"Number of Pages: {book.get('number_of_pages', 'N/A')}\\n\" \\\n",
        "                  f\"Publishers: {', '.join(book.get('publishers', ['N/A']))}\\n\" \\\n",
        "                  f\"Author Bio: {book.get('author_bio', 'No author biography available.')}\\n\" \\\n",
        "                  f\"Author Birth Date: {book.get('author_birth_date', 'N/A')}\\n\" \\\n",
        "                  f\"Author Death Date: {book.get('author_death_date', 'N/A')}\"\n",
        "\n",
        "        # Store useful metadata for source attribution\n",
        "        metadata = {\n",
        "            \"title\": book.get('title', 'N/A'),\n",
        "            \"author\": ', '.join(book.get('author_name', ['N/A'])),\n",
        "            \"olid\": book.get('key', '').replace('/works/', '') # Use work OLID as a stable ID\n",
        "        }\n",
        "        documents.append(Document(page_content=content, metadata=metadata))\n",
        "\n",
        "    if not documents:\n",
        "        raise ValueError(\"No documents generated to create FAISS index.\")\n",
        "\n",
        "    print(\"  Creating FAISS vector store...\")\n",
        "    vectorstore = FAISS.from_documents(documents, embeddings_model)\n",
        "    print(\"  FAISS vector store created.\")\n",
        "    return vectorstore\n"
      ],
      "metadata": {
        "id": "hhS59AUVYChq"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_search_terms_from_question(question: str, llm_model: ChatGoogleGenerativeAI, chat_history_messages: list) -> str:\n",
        "    \"\"\"\n",
        "    Uses LLM to extract relevant search terms from a user's question, considering chat history.\n",
        "    \"\"\"\n",
        "    # Format chat history for the prompt\n",
        "    history_str = \"\"\n",
        "    for msg in chat_history_messages:\n",
        "        # LangChain's ConversationBufferMemory stores messages as HumanMessage/AIMessage objects\n",
        "        if hasattr(msg, 'type') and hasattr(msg, 'content'):\n",
        "            history_str += f\"{msg.type.capitalize()}: {msg.content}\\n\"\n",
        "        # Fallback for older message formats if any\n",
        "        elif isinstance(msg, dict):\n",
        "            if 'human' in msg:\n",
        "                history_str += f\"Human: {msg['human']}\\n\"\n",
        "            if 'ai' in msg:\n",
        "                history_str += f\"AI: {msg['ai']}\\n\"\n",
        "\n",
        "\n",
        "    prompt = f\"\"\"Given the following conversation history and the new user question, identify the primary book title, author name, or series name that would be most effective for searching Open Library. If the question is about a general topic or genre, extract that.\n",
        "    Return only the most relevant single keyword or a short phrase for Open Library search. Do NOT include words like 'plot', 'storyline', 'summary', 'rating', 'review', 'what is the', 'tell me about' in the extracted terms.\n",
        "    If no specific book/author/series is mentioned, return a relevant genre or 'book'.\n",
        "\n",
        "    Chat History:\n",
        "    {history_str}\n",
        "\n",
        "    User Question: \"{question}\"\n",
        "\n",
        "    Search Term:\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = llm_model.invoke(prompt)\n",
        "        extracted_term = response.content.strip()\n",
        "\n",
        "        # Basic cleanup for common phrases that LLM might still include\n",
        "        extracted_term_lower = extracted_term.lower()\n",
        "        for phrase in [\"plot of\", \"storyline of\", \"summary of\", \"rating of\", \"review of\", \"what is the\", \"tell me about\"]:\n",
        "            if extracted_term_lower.startswith(phrase):\n",
        "                extracted_term = extracted_term_lower.replace(phrase, \"\", 1).strip()\n",
        "\n",
        "        if not extracted_term:\n",
        "            question_lower = question.lower()\n",
        "            if \"plot of\" in question_lower:\n",
        "                return question_lower.split(\"plot of\", 1)[1].strip().replace(\"?\", \"\").replace(\".\", \"\")\n",
        "            elif \"storyline of\" in question_lower:\n",
        "                return question_lower.split(\"storyline of\", 1)[1].strip().replace(\"?\", \"\").replace(\".\", \"\")\n",
        "            return \"book\" # Fallback\n",
        "        return extracted_term\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting search terms with LLM: {e}. Falling back to simple parsing.\")\n",
        "        # Fallback if LLM extraction fails\n",
        "        question_lower = question.lower()\n",
        "        if \"plot of\" in question_lower:\n",
        "            return question_lower.split(\"plot of\", 1)[1].strip().replace(\"?\", \"\").replace(\".\", \"\")\n",
        "        elif \"storyline of\" in question_lower:\n",
        "            return question_lower.split(\"storyline of\", 1)[1].strip().replace(\"?\", \"\").replace(\".\", \"\")\n",
        "        return \"book\" # Final fallback"
      ],
      "metadata": {
        "id": "W8ZYe_dGX-b9"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define custom prompt templates for the ConversationalRetrievalChain\n",
        "qa_prompt_template = \"\"\"\n",
        "You are a helpful chatbot that answers questions about books.\n",
        "Use ONLY the following pieces of context to answer the question at the end.\n",
        "If the information is not explicitly mentioned in the context, say \"I don't have specific information about that.\"\n",
        "Do not make up or infer information that is not directly stated in the context.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Chat History:\n",
        "{chat_history}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "QA_PROMPT = PromptTemplate(\n",
        "    template=qa_prompt_template,\n",
        "    input_variables=[\"context\", \"question\", \"chat_history\"]\n",
        ")"
      ],
      "metadata": {
        "id": "uz24XjJEYInf"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def main():\n",
        "    print(\"--- LibriQuery: Command-Line RAG Book Q&A System ---\")\n",
        "    print(\"Ask any question about books, and I'll try to answer based on Open Library data.\")\n",
        "    print(\"Type 'exit' to quit.\")\n",
        "\n",
        "    # Initialize LangChain components once\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GEMINI_API_KEY)\n",
        "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.7, google_api_key=GEMINI_API_KEY)\n",
        "    memory = ConversationBufferMemory(\n",
        "        memory_key=\"chat_history\",\n",
        "        return_messages=True,\n",
        "        output_key=\"answer\"\n",
        "    )\n",
        "\n",
        "    # Initialize vectorstore and conversation_chain outside the loop\n",
        "    # They will be updated dynamically\n",
        "    vectorstore = None\n",
        "    conversation_chain = None\n",
        "\n",
        "    # Store fetched books globally for the session\n",
        "    global_books_context = []\n",
        "\n",
        "    while True:\n",
        "        user_question = input(\"\\nYour Question (or 'exit' to quit): \")\n",
        "        if user_question.lower() == 'exit':\n",
        "            print(\"Exiting Q&A session. Goodbye!\")\n",
        "            break\n",
        "        if not user_question.strip():\n",
        "            print(\"Please enter a question.\")\n",
        "            continue\n",
        "\n",
        "        print(\"Understanding your question and searching for context...\")\n",
        "\n",
        "        # Step 1: Extract search terms from the user's question using LLM, considering history\n",
        "        # Pass the current chat history to the extraction function\n",
        "        search_terms = extract_search_terms_from_question(user_question, llm, memory.buffer_as_messages)\n",
        "        print(f\"Identified search terms: '{search_terms}'\")\n",
        "\n",
        "        # Step 2: Fetch initial book data from Open Library based on extracted terms\n",
        "        initial_search_results = fetch_open_library_data_for_context(search_terms)\n",
        "\n",
        "        # Check if new results are relevant enough to update context, or if we should rely on existing\n",
        "        should_update_context = False\n",
        "        if initial_search_results:\n",
        "            # Simple heuristic: if the search terms yielded results, update context\n",
        "            should_update_context = True\n",
        "        elif not vectorstore:\n",
        "            # If no new results AND no existing vectorstore, then we truly have no context\n",
        "            print(\"No books found matching your query for context. Please try a different question.\")\n",
        "            continue # Skip to next turn if no context at all\n",
        "\n",
        "        if should_update_context:\n",
        "            # Step 3: Enrich book data with more details from various Open Library APIs\n",
        "            print(\"  Enriching book data with more details from multiple Open Library APIs...\")\n",
        "            newly_fetched_books = []\n",
        "            for i, book in enumerate(initial_search_results):\n",
        "                print(f\"    Enriching book {i+1}/{len(initial_search_results)}: {book.get('title', 'N/A')}\")\n",
        "\n",
        "                # Fetch detailed work data\n",
        "                work_olid = book.get('key', '').replace('/works/', '')\n",
        "                if work_olid:\n",
        "                    detailed_work_data = fetch_detailed_work_data(work_olid)\n",
        "                    book.update(detailed_work_data)\n",
        "                    time.sleep(0.05) # Small delay\n",
        "\n",
        "                # Fetch author details if OLID is available from work data\n",
        "                author_ol_ids = book.get('authors_ol_ids', [])\n",
        "                if author_ol_ids:\n",
        "                    # For simplicity, just fetch details for the first author found\n",
        "                    first_author_olid = author_ol_ids[0]\n",
        "                    author_details = fetch_author_details(first_author_olid)\n",
        "                    book['author_bio'] = author_details.get('bio')\n",
        "                    book['author_birth_date'] = author_details.get('birth_date')\n",
        "                    book['author_death_date'] = author_details.get('death_date')\n",
        "                    book['author_subjects'] = author_details.get('author_subjects') # Add author's subjects\n",
        "                    time.sleep(0.05) # Small delay\n",
        "\n",
        "                # Fetch ISBN-specific data if ISBNs are available\n",
        "                isbns = book.get('isbn', [])\n",
        "                if isbns:\n",
        "                    # Use the first ISBN found for detailed lookup\n",
        "                    first_isbn = isbns[0]\n",
        "                    isbn_data = fetch_book_by_isbn(first_isbn)\n",
        "                    book.update(isbn_data) # Merge ISBN-specific data\n",
        "                    time.sleep(0.05) # Small delay\n",
        "\n",
        "                # Construct cover URL\n",
        "                book['cover_url'] = get_cover_url(olid=book.get('cover_edition_key', '').replace('/books/', '') or book.get('olid'), isbn=isbns[0] if isbns else None)\n",
        "\n",
        "                newly_fetched_books.append(book)\n",
        "\n",
        "            # Step 4: Update FAISS index with new data\n",
        "            try:\n",
        "                if vectorstore is None:\n",
        "                    # First time, create the index\n",
        "                    vectorstore = create_faiss_index(newly_fetched_books, embeddings)\n",
        "                else:\n",
        "                    # Subsequent times, add new documents to the existing index\n",
        "                    documents_to_add = []\n",
        "                    for book in newly_fetched_books:\n",
        "                        content = f\"Title: {book.get('title', 'N/A')}\\n\" \\\n",
        "                                  f\"Author: {', '.join(book.get('author_name', ['N/A']))}\\n\" \\\n",
        "                                  f\"Description: {book.get('description', book.get('first_sentence', ['No description available.'])[0])}\\n\" \\\n",
        "                                  f\"Genres: {', '.join(book.get('subjects', book.get('subject', ['N/A'])))}\\n\" \\\n",
        "                                  f\"Publish Date: {book.get('publish_date', 'N/A')}\\n\" \\\n",
        "                                  f\"Number of Pages: {book.get('number_of_pages', 'N/A')}\\n\" \\\n",
        "                                  f\"Publishers: {', '.join(book.get('publishers', ['N/A']))}\\n\" \\\n",
        "                                  f\"Author Bio: {book.get('author_bio', 'No author biography available.')}\\n\" \\\n",
        "                                  f\"Author Birth Date: {book.get('author_birth_date', 'N/A')}\\n\" \\\n",
        "                                  f\"Author Death Date: {book.get('author_death_date', 'N/A')}\"\n",
        "                        metadata = {\n",
        "                            \"title\": book.get('title', 'N/A'),\n",
        "                            \"author\": ', '.join(book.get('author_name', ['N/A'])),\n",
        "                            \"olid\": book.get('key', '').replace('/works/', '')\n",
        "                        }\n",
        "                        documents_to_add.append(Document(page_content=content, metadata=metadata))\n",
        "\n",
        "                    if documents_to_add:\n",
        "                        print(f\"  Adding {len(documents_to_add)} new documents to FAISS vector store...\")\n",
        "                        vectorstore.add_documents(documents_to_add)\n",
        "                        print(\"  Documents added.\")\n",
        "\n",
        "                retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
        "\n",
        "                # Re-initialize conversation chain with the potentially updated retriever\n",
        "                conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "                    llm=llm,\n",
        "                    retriever=retriever,\n",
        "                    memory=memory,\n",
        "                    combine_docs_chain_kwargs={\"prompt\": QA_PROMPT},\n",
        "                    return_source_documents=True\n",
        "                )\n",
        "                print(\"  RAG chain initialized/updated with new context.\")\n",
        "\n",
        "            except ValueError as e:\n",
        "                print(f\"Error during FAISS index creation/update: {e}\")\n",
        "                print(\"Please ensure your Gemini API key is correct and try a different query.\")\n",
        "                continue # Skip to next turn if index creation fails\n",
        "            except Exception as e:\n",
        "                print(f\"An unexpected error occurred during FAISS index creation/update: {e}\")\n",
        "                print(\"Please try again.\")\n",
        "        else:\n",
        "            print(\"  No new relevant books found. Using existing context for Q&A.\")\n",
        "            if conversation_chain is None:\n",
        "                print(\"  No existing context available. Please try a different initial query.\")\n",
        "                continue # Skip if no context at all\n",
        "\n",
        "        # Step 5: Answer the user's question using the RAG chain\n",
        "        print(\"Generating answer using RAG chain...\")\n",
        "        try:\n",
        "            result = conversation_chain({\"question\": user_question})\n",
        "            answer = result[\"answer\"]\n",
        "            source_documents = result[\"source_documents\"]\n",
        "\n",
        "            print(\"\\nAnswer:\", answer)\n",
        "            print(\"\\nSources:\")\n",
        "            if source_documents:\n",
        "                for i, doc in enumerate(source_documents[:3]): # Show only top 3 sources\n",
        "                    # Displaying metadata for context\n",
        "                    source_title = doc.metadata.get('title', 'Unknown Title')\n",
        "                    source_author = doc.metadata.get('author', 'Unknown Author')\n",
        "                    print(f\"Source {i + 1}: Title: '{source_title}', Author: '{source_author}'\")\n",
        "            else:\n",
        "                print(\"No specific sources found in the retrieved context.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating answer with RAG chain: {e}\")\n",
        "            print(\"Please try again.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkTunMxnTUs8",
        "outputId": "050b14a1-12fc-4c88-f69a-24a106a5a015"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- LibriQuery: Command-Line RAG Book Q&A System ---\n",
            "Ask any question about books, and I'll try to answer based on Open Library data.\n",
            "Type 'exit' to quit.\n",
            "\n",
            "Your Question (or 'exit' to quit): exit\n",
            "Exiting Q&A session. Goodbye!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}